{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import timm\n",
    "import pandas as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.amp import GradScaler\n",
    "import cv2\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.autograd import Variable\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pairs = pl.read_csv('data/raw/pairs_list.csv')\n",
    "paths_embeds = pl.read_csv('data/raw/paths_embeds.csv')['image_path']\n",
    "real_embeds = np.load('data/raw/real_embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MCSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path, target, imsize = 112):\n",
    "        self.image_path = image_path\n",
    "        self.target = target\n",
    "        self.image_size = imsize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def resize(self, img, interp):\n",
    "        return  cv2.resize(\n",
    "            img, (self.image_size, self.image_size), interpolation=interp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_path[idx]\n",
    "        target = self.target[idx]\n",
    "        img = cv2.imread(f'data/raw/train/{path}')\n",
    "        img = cv2.resize(\n",
    "            img, (self.image_size, self.image_size), interpolation= cv2.INTER_LINEAR)\n",
    "\n",
    "        img = (img / 255.) - 0.5\n",
    "        img = np.transpose(img,(2,0,1)).astype(np.float32)\n",
    "        img = torch.from_numpy(img)\n",
    "        target = torch.from_numpy(target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name,):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.timm_ = timm.create_model( model_name, global_pool='', num_classes=0, in_chans=3)\n",
    "        output_features = self.timm_(torch.zeros((1, 3, 112, 112))).shape[1]\n",
    "        self.norm = nn.BatchNorm1d(output_features)\n",
    "    def forward(self, x):\n",
    "        out_ = self.norm(self.timm_(x).mean(dim=(2, 3)))\n",
    "        out_ = F.normalize(out_)\n",
    "        return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_predict(model, val_loader, val_target, loss_func, DEVICE = 'cuda'):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    average_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_number,  (img, target)  in enumerate(val_loader):\n",
    "            img = img.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(img)\n",
    "                loss = loss_func(outputs, target)\n",
    "\n",
    "            average_loss += loss.cpu().detach().numpy()\n",
    "            preds += [outputs.to('cpu').numpy()]\n",
    "    preds = np.concatenate(preds)\n",
    "    print('MSE: ', ((preds -  np.array(val_target)) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6beaf0487e4439ae8538c29bcbd4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0033730713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7498fcdb2334cb199d345a091b86d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0027727515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d614ac412d9d4e7e9701a5e0d60283b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0023166751\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948f82cba94f492bae21b820524ef9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.00211011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5884fd77894e17991774d6dcf7fa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0019993677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4d4ec74ebd4a9abeff4f93fd22952a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0018932617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6cc51e90934e27be47a261a0f9611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0018474418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cc2b2b462247e8b761d545cfde6603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0018223765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a24c678bcf24960827ebb60398ff82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017786095\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc15400ee90b4b709f36e19bc36c8fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017848207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96ae9f7edb644b99dec7d2428aebaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017671624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba7cf48aaa44fef99b20913fb1128c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017630995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af34813691c4087a918653d28f8556c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017570672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915f57650fc84855b103084dcbae282a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017556978\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9be4af6f3543929529be78749be166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017536711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb94fdcd1124c769f5a4003e36c236d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017562159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2154b7a2774b67bcf3388b5d7610f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0017575773\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 64\n",
    "valid_batch_size = 64\n",
    "epochs = 17\n",
    "lr = 3.22e-4\n",
    "clip_grad_norm = 15.28\n",
    "DEVICE = 'cuda'\n",
    "params_train = {'batch_size': batch_size, 'shuffle': True, 'drop_last': True, 'num_workers': 2}\n",
    "params_val = {'batch_size': batch_size, 'shuffle': False, 'drop_last': False, 'num_workers': 2}\n",
    "\n",
    "train_path = [x for i, x in enumerate(paths_embeds) if i % 5 != 0 ]\n",
    "train_target = [x for i, x in enumerate(real_embeds) if i % 5 != 0 ]\n",
    "\n",
    "\n",
    "val_path = [x for i, x in enumerate(paths_embeds) if i % 5 == 0 ]\n",
    "val_target = [x for i, x in enumerate(real_embeds) if i % 5 == 0 ]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(MCSDataset(train_path, train_target), **params_train)\n",
    "val_loader = torch.utils.data.DataLoader(MCSDataset(val_path, val_target), **params_val)\n",
    "num_lbl = 2000\n",
    "\n",
    "model = Model('resnet18').cuda()\n",
    "num_train_steps = int(len(train_loader) / batch_size  * epochs)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "scaler = GradScaler('cuda')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs, 1e-6)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    average_loss = 0\n",
    "    tk0 = tqdm(enumerate(train_loader), total = len(train_loader))\n",
    "    for batch_number,  (img, target)  in tk0:\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        # continue\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(img)\n",
    "            loss = loss_func(outputs, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        average_loss += loss.cpu().detach().numpy()\n",
    "        tk0.set_postfix(loss=average_loss / (batch_number + 1),lr = scheduler.get_last_lr()[0], stage=\"train\", epoch = epoch)\n",
    "    make_predict(model, val_loader,val_target, loss_func)\n",
    "    \n",
    "std_m = model.state_dict()\n",
    "!mkdir -p checkpoints/baseline\n",
    "torch.save(std_m, f'checkpoints/baseline/model_student.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_img(path, image_size = 112):\n",
    "    img = cv2.imread(f'data/raw/train/{path}')\n",
    "    img_ = cv2.resize(\n",
    "        img, (image_size, image_size), interpolation= cv2.INTER_LINEAR)\n",
    "    img = (img_ / 255.) - 0.5\n",
    "    img = np.transpose(img,(2,0,1)).astype(np.float32)\n",
    "    img = torch.from_numpy(img)\n",
    "    return img, img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d241fd0ea53b43e6b0b401417a54c54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_iter = 10\n",
    "loss = nn.MSELoss()\n",
    "eps = 1e-3\n",
    "attacked_img_dict = {}\n",
    "\n",
    "\n",
    "for sour, targ in tqdm(zip(pairs['source_imgs'], pairs['target_imgs']), total=len(pairs['source_imgs'])):\n",
    "\n",
    "    target_descriptors = np.ones((5, 512), dtype=np.float32)\n",
    "    targ = targ.split('|')\n",
    "    sour = sour.split('|')\n",
    "\n",
    "    list_tagt_img = []\n",
    "    for i, t in enumerate(targ):\n",
    "        img, orig_tgt = read_img(t)\n",
    "        list_tagt_img += [orig_tgt]\n",
    "        img = img.unsqueeze(0).cuda(non_blocking = True)\n",
    "        res = model(Variable(img, requires_grad=False)).data.cpu().numpy().squeeze()\n",
    "        target_descriptors[i] = res\n",
    "\n",
    "    for ii, s in enumerate(sour): \n",
    "        img, orig_img = read_img(s)\n",
    "        img = img.unsqueeze(0).cuda(non_blocking = True)\n",
    "        input_var  = Variable(img, requires_grad=True)\n",
    "        attacked_img = orig_img\n",
    "        for iter_number in (range(max_iter)):\n",
    "            adv_noise = torch.zeros((3,112,112)).cuda(non_blocking = True)\n",
    "            for tg in target_descriptors:\n",
    "                target_out = Variable(torch.from_numpy(tg).unsqueeze(0).cuda(non_blocking=True), requires_grad=False)\n",
    "                input_var.grad = None\n",
    "                out = model(input_var)\n",
    "                calc_loss = loss(out, target_out)\n",
    "                calc_loss.backward()\n",
    "                noise = eps * torch.sign(input_var.grad.data)\\\n",
    "                                    .squeeze()\n",
    "                adv_noise = adv_noise + noise\n",
    "\n",
    "            input_var.data = input_var.data - adv_noise\n",
    "\n",
    "            changed_img = input_var.data.cpu().squeeze()\n",
    "            changed_img = ((changed_img + 0.5) * 255)\n",
    "            changed_img[changed_img < 0] = 0\n",
    "            changed_img[changed_img > 255] = 255\n",
    "            changed_img = np.transpose(changed_img.numpy(), (1, 2, 0)).astype(np.int16)\n",
    "            ssim_score = ssim(orig_img, changed_img, channel_axis=2, data_range = 256)\n",
    "            if ssim_score < 0.95:\n",
    "                break\n",
    "            else:\n",
    "                attacked_img = changed_img\n",
    "        attacked_img_dict[s] = attacked_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pl.read_csv('data/raw/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62d2a7afce0435897fa80f59a226601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_submission_df = pl.DataFrame()\n",
    "sample_submission_df['Id'] = sample_submission['Id']\n",
    "\n",
    "result = []\n",
    "for id_ in tqdm(sample_submission_df['Id']):\n",
    "    result += [ '|'.join([str(i) for i in attacked_img_dict[id_].flatten().tolist()])  ]\n",
    "sample_submission_df['Target'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission_df.to_csv('data/submissions/sample_submission_baseline.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# METRIC FUNCTION\n",
    "\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "# \n",
    "# class MCSDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, image_path,  imsize = 112):\n",
    "#         self.image_path = image_path\n",
    "#         self.image_size = imsize\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_path)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img = self.image_path[idx]\n",
    "#         img = (img / 255.) - 0.5\n",
    "#         img = np.transpose(img,(2,0,1)).astype(np.float32)\n",
    "#         img = torch.from_numpy(img)\n",
    "\n",
    "#         return img\n",
    "\n",
    "        \n",
    "# model = Model().eval()\n",
    "\n",
    "# pairs = pd.read_csv('data/raw/pairs_list.csv')\n",
    "# paths_embeds = pd.read_csv('data/raw/paths_embeds.csv')\n",
    "# embeds = np.load('data/raw/real_embeds.npy')\n",
    "# sample_submission = pd.read_csv('data/raw/sample_submission.csv')\n",
    "# dict_embeds = {x:i for i,x in enumerate(paths_embeds['image_path'])}\n",
    "\n",
    "# imgs_ = [np.array([int(i) for i in x.split('|')]).reshape((112, 112, 3)) for x in submission['Target']]\n",
    "\n",
    "# dict_ss_ids = {x:i for i,x in enumerate(sample_submission['Id'])}\n",
    "# for i, ids in enumerate(submission['Id']):\n",
    "#     val = sample_submission['Target'][dict_ss_ids[ids]]\n",
    "#     val = np.array([int(i) for i in val.split('|')]).reshape((112, 112, 3))\n",
    "#     sim_ = ssim(imgs_[i], val, channel_axis=2, data_range = 256)\n",
    "#     if sim_ < 0.95:\n",
    "#         return -1\n",
    "        \n",
    "# params_val = {'batch_size': 64, 'shuffle': False, 'drop_last': False, 'num_workers': 2}\n",
    "# imgs_path = os.listdir('/kaggle/input/ioai-contest-2') \n",
    "# val_loader = torch.utils.data.DataLoader(MCSDataset(imgs_), **params_val)\n",
    "\n",
    "# embeds_sourse = []\n",
    "# with torch.no_grad():\n",
    "#     for batch_number,  img  in tqdm(enumerate(val_loader)):\n",
    "#         outputs = model(img, None, train = False)\n",
    "#         embeds_sourse += [outputs]\n",
    "# embeds_sourse = np.concatenate(embeds_sourse)\n",
    "\n",
    "# dict_sours = {x:i for i,x in enumerate(submission['Id'])}\n",
    "# all_paths = set(submission['Id'])\n",
    "\n",
    "# all_score = []\n",
    "# for sour, targ in zip(pairs['source_imgs'], pairs['target_imgs']):\n",
    "\n",
    "#     sour = sour.split('|')\n",
    "#     targ = targ.split('|')\n",
    "\n",
    "#     if sour[0] in all_paths:\n",
    "#         score = []\n",
    "#         for s in sour:\n",
    "#             for t in targ:\n",
    "#                 if t != s:\n",
    "#                     score += [((embeds[dict_embeds[t]] - embeds_sourse[dict_sours[s]]) ** 2).sum() ** (1/2)]\n",
    "                    \n",
    "#         score = np.mean(score)\n",
    "#         all_score += [score]\n",
    "\n",
    "# score = np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
