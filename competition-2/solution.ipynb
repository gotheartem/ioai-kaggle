{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-02T17:00:02.218885Z",
     "iopub.status.busy": "2025-03-02T17:00:02.218616Z",
     "iopub.status.idle": "2025-03-02T17:00:15.033050Z",
     "shell.execute_reply": "2025-03-02T17:00:15.032291Z",
     "shell.execute_reply.started": "2025-03-02T17:00:02.218867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import timm\n",
    "import pandas as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.amp import GradScaler\n",
    "import cv2\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.autograd import Variable\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch.cuda.amp as amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:00:15.034322Z",
     "iopub.status.busy": "2025-03-02T17:00:15.033875Z",
     "iopub.status.idle": "2025-03-02T17:00:15.047899Z",
     "shell.execute_reply": "2025-03-02T17:00:15.047320Z",
     "shell.execute_reply.started": "2025-03-02T17:00:15.034295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:00:15.049921Z",
     "iopub.status.busy": "2025-03-02T17:00:15.049727Z",
     "iopub.status.idle": "2025-03-02T17:00:15.540114Z",
     "shell.execute_reply": "2025-03-02T17:00:15.539221Z",
     "shell.execute_reply.started": "2025-03-02T17:00:15.049904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pairs = pl.read_csv('data/raw/pairs_list.csv')\n",
    "paths_embeds = pl.read_csv('data/raw/paths_embeds.csv')['image_path']\n",
    "real_embeds = np.load('data/raw/real_embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:00:15.541996Z",
     "iopub.status.busy": "2025-03-02T17:00:15.541705Z",
     "iopub.status.idle": "2025-03-02T17:00:15.547677Z",
     "shell.execute_reply": "2025-03-02T17:00:15.546928Z",
     "shell.execute_reply.started": "2025-03-02T17:00:15.541974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MCSDataset(torch.utils.data.Dataset):\n",
    "    # Initialize the dataset with image paths, target labels, and an optional image size (default is 112)\n",
    "    def __init__(self, image_path, target, imsize=112):\n",
    "        self.image_path = image_path  # List of image file names or paths.\n",
    "        self.target = target          # List of target labels or descriptors corresponding to the images.\n",
    "        self.image_size = imsize      # Desired size (width and height) to which all images will be resized.\n",
    "\n",
    "    # Return the number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    # A helper function to resize an image using a specified interpolation method.\n",
    "    # This function is not used directly in __getitem__, but it provides an example of how to call cv2.resize with custom interpolation.\n",
    "    # def resize(self, img, interp):\n",
    "    #     return cv2.resize(\n",
    "    #         img, (self.image_size, self.image_size), interpolation=interp)\n",
    "\n",
    "    # Retrieve the image and target at index 'idx'\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the image file name/path and the corresponding target using the provided index.\n",
    "        path = self.image_path[idx]\n",
    "        target = self.target[idx]\n",
    "        \n",
    "        # Read the image from the specified path.\n",
    "        # It assumes that images are stored in the 'imgs/train/' directory.\n",
    "        img = cv2.imread(f'data/raw/train/{path}')\n",
    "        \n",
    "        # Resize the image to the desired dimensions using linear interpolation.\n",
    "        img = cv2.resize(\n",
    "            img, (self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Normalize the image pixels:\n",
    "        # - Convert pixel values from [0, 255] to [0, 1] by dividing by 255.\n",
    "        # - Then shift the range to [-0.5, 0.5] by subtracting 0.5.\n",
    "        img = (img / 255.) - 0.5\n",
    "        \n",
    "        # Change the image array from shape (height, width, channels) to (channels, height, width)\n",
    "        # This is necessary because PyTorch models expect the channels-first format.\n",
    "        img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n",
    "        \n",
    "        # Convert the numpy image array to a PyTorch tensor.\n",
    "        img = torch.from_numpy(img)\n",
    "        \n",
    "        # Convert the target (assumed to be a numpy array) to a PyTorch tensor.\n",
    "        target = torch.from_numpy(target)\n",
    "\n",
    "        # Return the processed image and its corresponding target.\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:00:15.548786Z",
     "iopub.status.busy": "2025-03-02T17:00:15.548509Z",
     "iopub.status.idle": "2025-03-02T17:00:15.570255Z",
     "shell.execute_reply": "2025-03-02T17:00:15.569423Z",
     "shell.execute_reply.started": "2025-03-02T17:00:15.548760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        # Initialize the parent class (nn.Module)\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save the model name (could be used for logging or further customization)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Create a backbone model from the timm library.\n",
    "        # - model_name: Specifies which pre-trained model to use.\n",
    "        # - global_pool='': Disables automatic global pooling.\n",
    "        # - num_classes=0: Removes the classification head.\n",
    "        # - in_chans=3: Specifies that the model expects 3-channel (RGB) input images.\n",
    "        self.timm_ = timm.create_model(model_name, global_pool='', num_classes=0, in_chans=3)\n",
    "        \n",
    "        # Determine the number of output features from the backbone model:\n",
    "        # Pass a dummy tensor through the model to check its output shape.\n",
    "        # The dummy tensor has shape (1, 3, 112, 112), matching the expected input size.\n",
    "        output_features = self.timm_(torch.zeros((1, 3, 112, 112))).shape[1]\n",
    "        \n",
    "        # Create a 1D Batch Normalization layer to normalize the extracted features.\n",
    "        # This layer will have as many features as the output of the backbone.\n",
    "        self.norm = nn.BatchNorm1d(output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input 'x' through the backbone model (timm_)\n",
    "        # The output is assumed to be a feature map of shape (batch_size, channels, height, width)\n",
    "        features = self.timm_(x)\n",
    "        \n",
    "        # Apply spatial average pooling by taking the mean over the height and width dimensions.\n",
    "        # This converts the feature maps into a 1D feature vector per sample.\n",
    "        pooled_features = features.mean(dim=(2, 3))\n",
    "        \n",
    "        # Normalize the pooled features using the Batch Normalization layer.\n",
    "        normed_features = self.norm(pooled_features)\n",
    "        \n",
    "        # Finally, apply L2 normalization (using F.normalize) to ensure the feature vector has unit norm.\n",
    "        # This is common in face recognition systems to facilitate distance-based comparisons.\n",
    "        out_ = F.normalize(normed_features)\n",
    "        \n",
    "        # Return the final normalized feature vector.\n",
    "        return out_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:00:15.571253Z",
     "iopub.status.busy": "2025-03-02T17:00:15.571002Z",
     "iopub.status.idle": "2025-03-02T17:00:15.590897Z",
     "shell.execute_reply": "2025-03-02T17:00:15.590176Z",
     "shell.execute_reply.started": "2025-03-02T17:00:15.571233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_predict(model, val_loader, val_target, loss_func, DEVICE='cuda'):\n",
    "    # List to accumulate predictions for all batches\n",
    "    preds = []\n",
    "    \n",
    "    # Set the model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize variable to accumulate the loss over all batches\n",
    "    average_loss = 0\n",
    "    \n",
    "    # Disable gradient calculation for inference to save memory and computations\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation DataLoader\n",
    "        for batch_number, (img, target) in enumerate(val_loader):\n",
    "            # Move input images and targets to the specified device (e.g., GPU)\n",
    "            img = img.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            \n",
    "            # Use automatic mixed precision for efficiency during inference\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                # Forward pass: compute model outputs for the batch of images\n",
    "                outputs = model(img)\n",
    "                \n",
    "                # Calculate loss between the model outputs and the true targets using the given loss function\n",
    "                loss = loss_func(outputs, target)\n",
    "            \n",
    "            # Accumulate the loss: move loss to CPU, detach from the graph, convert to numpy, then add to total\n",
    "            average_loss += loss.cpu().detach().numpy()\n",
    "            \n",
    "            # Convert the outputs to a numpy array on the CPU and add them to the predictions list\n",
    "            preds += [outputs.to('cpu').numpy()]\n",
    "    \n",
    "    # Concatenate predictions from all batches into a single numpy array\n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    # Calculate the Mean Squared Error (MSE) between the predictions and the actual target values,\n",
    "    # then print it. This is computed as the mean of squared differences.\n",
    "    print('MAE: ', ((preds - np.array(val_target)) ** 2).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:00:15.591999Z",
     "iopub.status.busy": "2025-03-02T17:00:15.591727Z",
     "iopub.status.idle": "2025-03-02T17:05:54.338735Z",
     "shell.execute_reply": "2025-03-02T17:05:54.337344Z",
     "shell.execute_reply.started": "2025-03-02T17:00:15.591971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.0027296157\n",
      "MAE:  0.0022350024\n",
      "MAE:  0.0019997736\n",
      "MAE:  0.0018457756\n",
      "MAE:  0.0017663452\n",
      "MAE:  0.001702216\n",
      "MAE:  0.0016507987\n",
      "MAE:  0.0016446318\n",
      "MAE:  0.0016257827\n",
      "MAE:  0.0016183533\n",
      "MAE:  0.0016124534\n",
      "MAE:  0.0016108073\n",
      "MAE:  0.0015928042\n",
      "MAE:  0.0015997468\n",
      "MAE:  0.0015924731\n",
      "MAE:  0.0015874435\n",
      "MAE:  0.0015853907\n",
      "MAE:  0.0015822836\n",
      "MAE:  0.0015801914\n",
      "MAE:  0.0015789379\n",
      "MAE:  0.0015758416\n",
      "MAE:  0.0015791811\n",
      "MAE:  0.0015773601\n",
      "MAE:  0.0015774049\n",
      "MAE:  0.0015787614\n",
      "MAE:  0.0015801645\n",
      "MAE:  0.0015793251\n",
      "MAE:  0.0015803353\n",
      "MAE:  0.0015810999\n",
      "MAE:  0.0015818949\n"
     ]
    }
   ],
   "source": [
    "# Clean up any unused objects and free up GPU memory\n",
    "gc.collect()                         # Collect garbage from Python memory\n",
    "torch.cuda.empty_cache()             # Empty unused cached memory on the GPU\n",
    "\n",
    "# Hyperparameters and device configuration\n",
    "batch_size = 64                      # Training batch size\n",
    "valid_batch_size = 64                # Validation batch size (can be same as training)\n",
    "epochs = 30                          # Number of training epochs\n",
    "lr = 3e-3                            # Learning rate for the optimizer\n",
    "clip_grad_norm = 15.8              # Maximum norm for gradient clipping\n",
    "DEVICE = 'cuda'                      # Device to use for training (GPU)\n",
    "\n",
    "# DataLoader parameters for training and validation datasets\n",
    "params_train = {\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': True,                 # Shuffle the training data each epoch\n",
    "    'drop_last': True,               # Drop last incomplete batch if dataset size is not divisible by batch_size\n",
    "    'num_workers': 2                 # Number of subprocesses to use for data loading\n",
    "}\n",
    "params_val = {\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': False,                # Do not shuffle validation data\n",
    "    'drop_last': False,              # Do not drop the last batch; useful for evaluation\n",
    "    'num_workers': 2\n",
    "}\n",
    "\n",
    "# Split the dataset into training and validation sets using modulo indexing.\n",
    "# Every 5th element is used for validation, the rest for training.\n",
    "train_path = [x for i, x in enumerate(paths_embeds) if i % 5 != 0 ]\n",
    "train_target = [x for i, x in enumerate(real_embeds) if i % 5 != 0 ]\n",
    "val_path = [x for i, x in enumerate(paths_embeds) if i % 5 == 0 ]\n",
    "val_target = [x for i, x in enumerate(real_embeds) if i % 5 == 0 ]\n",
    "\n",
    "# Create PyTorch DataLoaders for training and validation datasets using the custom MCSDataset\n",
    "train_loader = torch.utils.data.DataLoader(MCSDataset(train_path, train_target), **params_train)\n",
    "val_loader = torch.utils.data.DataLoader(MCSDataset(val_path, val_target), **params_val)\n",
    "\n",
    "# Number of labels (not used further in the provided snippet but might be used elsewhere)\n",
    "num_lbl = 2000\n",
    "\n",
    "# Initialize the model using a specified architecture ('resnet18') and move it to the GPU\n",
    "model = Model('resnet34').cuda()\n",
    "\n",
    "# Calculate the number of training steps (not used explicitly in this snippet)\n",
    "num_train_steps = int(len(train_loader) / batch_size * epochs)\n",
    "\n",
    "# Define the loss function (Mean Absolute Error loss) for regression-like outputs\n",
    "loss_func = torch.nn.L1Loss()\n",
    "\n",
    "# Set up a gradient scaler for mixed precision training (improves efficiency on GPU)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# Create an AdamW optimizer to update model parameters with the given learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "\n",
    "# Set up a cosine annealing learning rate scheduler with a minimal learning rate of 5e-7\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs, 5e-9)\n",
    "# Training loop for the specified number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode (activates dropout, batch norm, etc.)\n",
    "    average_loss = 0  # Initialize the loss accumulator for the epoch\n",
    "\n",
    "    # Use tqdm to create a progress bar for the training loop over batches\n",
    "    # tk0 = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for (img, target) in train_loader:\n",
    "        optimizer.zero_grad()  # Zero out gradients before each batch to avoid accumulation\n",
    "        img = img.to(DEVICE)   # Move image batch to the GPU\n",
    "        target = target.to(DEVICE)  # Move target batch to the GPU\n",
    "\n",
    "        # Use automatic mixed precision context to improve performance on GPU\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(img)  # Forward pass: compute model outputs for the batch\n",
    "            loss = loss_func(outputs, target)  # Compute loss between outputs and target\n",
    "\n",
    "        # Scale the loss and perform backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale gradients and perform gradient clipping to stabilize training\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "\n",
    "        # Step the optimizer and update the scaler for mixed precision training\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Accumulate loss (move to CPU and detach from computation graph for safe numpy conversion)\n",
    "        average_loss += loss.cpu().detach().numpy()\n",
    "        # Update progress bar with current average loss, current learning rate, and epoch number\n",
    "        # tk0.set_postfix(loss=average_loss / (batch_number + 1),\n",
    "        #                 lr=scheduler.get_last_lr()[0],\n",
    "        #                 stage=\"train\",\n",
    "        #                 epoch=epoch)\n",
    "    \n",
    "    # Evaluate the model on the validation dataset after each epoch\n",
    "    make_predict(model, val_loader, val_target, loss_func)\n",
    "    \n",
    "# Save the final model state dictionary after training completes\n",
    "std_m = model.state_dict()\n",
    "!mkdir -p checkpoints/solution\n",
    "torch.save(std_m, f'checkpoints/solution/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:05:54.340407Z",
     "iopub.status.busy": "2025-03-02T17:05:54.340090Z",
     "iopub.status.idle": "2025-03-02T17:05:54.345308Z",
     "shell.execute_reply": "2025-03-02T17:05:54.344652Z",
     "shell.execute_reply.started": "2025-03-02T17:05:54.340380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_img(path, image_size=112):\n",
    "    # Read the image from the specified file path.\n",
    "    # The file is expected to be in the 'imgs/train/' directory.\n",
    "    img = cv2.imread(f'data/raw/train/{path}')\n",
    "    \n",
    "    # Resize the image to the desired dimensions (image_size x image_size)\n",
    "    # using linear interpolation for smooth resizing.\n",
    "    img_ = cv2.resize(img, (image_size, image_size), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Normalize the resized image:\n",
    "    # - Convert pixel values from the range [0, 255] to [0, 1] by dividing by 255.\n",
    "    # - Shift the range to [-0.5, 0.5] by subtracting 0.5.\n",
    "    img = (img_ / 255.) - 0.5\n",
    "    \n",
    "    # Change the image layout from (height, width, channels) to (channels, height, width)\n",
    "    # which is the expected format for PyTorch models.\n",
    "    img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n",
    "    \n",
    "    # Convert the numpy array into a PyTorch tensor.\n",
    "    img = torch.from_numpy(img)\n",
    "    \n",
    "    # Return the processed image tensor and the resized image (in its original numpy format)\n",
    "    return img, img_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:05:54.347793Z",
     "iopub.status.busy": "2025-03-02T17:05:54.347597Z",
     "iopub.status.idle": "2025-03-02T17:43:24.741495Z",
     "shell.execute_reply": "2025-03-02T17:43:24.740572Z",
     "shell.execute_reply.started": "2025-03-02T17:05:54.347777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb24a80f491b47b891c8b7a252c93ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set maximum number of iterations for the adversarial attack optimization\n",
    "max_iter = 10\n",
    "\n",
    "# Define the loss function (Mean Squared Error) for comparing descriptors\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Define the step size (epsilon) for the adversarial perturbation\n",
    "eps = 1e-3\n",
    "\n",
    "# Dictionary to store the final attacked images keyed by the source image path/name\n",
    "attacked_img_dict = {}\n",
    "\n",
    "# Iterate over each pair of source and target images using tqdm for progress visualization.\n",
    "# Each element in pairs['source_imgs'] and pairs['target_imgs'] is assumed to be a string\n",
    "# containing multiple image paths separated by '|'.\n",
    "for sour, targ in tqdm(zip(pairs['source_imgs'], pairs['target_imgs']), total=len(pairs['source_imgs'])):\n",
    "    \n",
    "    # Initialize an array to store the target descriptors for the 5 target images.\n",
    "    # The shape (5, 512) assumes each descriptor has 512 dimensions.\n",
    "    target_descriptors = np.ones((5, 512), dtype=np.float32)\n",
    "    \n",
    "    # Split the target and source image strings into individual file names/paths.\n",
    "    targ = targ.split('|')\n",
    "    sour = sour.split('|')\n",
    "\n",
    "    # List to optionally collect the original target images (might be used for debugging/visualization)\n",
    "    list_tagt_img = []\n",
    "    \n",
    "    # Process each target image to compute its descriptor.\n",
    "    for i, t in enumerate(targ):\n",
    "        # Read the image using the read_img function.\n",
    "        # img is the preprocessed tensor and orig_tgt is the resized original image.\n",
    "        img, orig_tgt = read_img(t)\n",
    "        \n",
    "        # Append the original target image to the list.\n",
    "        list_tagt_img += [orig_tgt]\n",
    "        \n",
    "        # Add a batch dimension and move the image tensor to GPU.\n",
    "        img = img.unsqueeze(0).cuda(non_blocking=True)\n",
    "        \n",
    "        # Pass the image through the model (without gradients) to get its descriptor.\n",
    "        # Variable() wraps the tensor, though requires_grad is False here.\n",
    "        res = model(Variable(img, requires_grad=False)).data.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Store the computed descriptor in the target_descriptors array.\n",
    "        target_descriptors[i] = res\n",
    "\n",
    "    # For each source image, perform the adversarial attack.\n",
    "    for ii, s in enumerate(sour):\n",
    "        # Read the source image; orig_img is the resized original image.\n",
    "        img, orig_img = read_img(s)\n",
    "        \n",
    "        # Add batch dimension and move to GPU.\n",
    "        img = img.unsqueeze(0).cuda(non_blocking=True)\n",
    "        \n",
    "        # Wrap the image tensor in a Variable and enable gradient computation.\n",
    "        input_var = Variable(img, requires_grad=True)\n",
    "        \n",
    "        # Initialize attacked_img as the original image (will update if attack succeeds).\n",
    "        attacked_img = orig_img\n",
    "        \n",
    "        # Perform iterative optimization for the adversarial attack.\n",
    "        for iter_number in range(max_iter):\n",
    "            # Initialize the adversarial noise accumulator as zeros.\n",
    "            adv_noise = torch.zeros((3, 112, 112)).cuda(non_blocking=True)\n",
    "            \n",
    "            # Loop over each target descriptor to compute gradients for each.\n",
    "            for tg in target_descriptors:\n",
    "                # Convert the current target descriptor into a tensor Variable on GPU.\n",
    "                # No gradient is required for target_out.\n",
    "                target_out = Variable(torch.from_numpy(tg).unsqueeze(0).cuda(non_blocking=True), requires_grad=False)\n",
    "                \n",
    "                # Clear any existing gradients in input_var.\n",
    "                input_var.grad = None\n",
    "                \n",
    "                # Forward pass: get the current descriptor of the (possibly perturbed) source image.\n",
    "                out = model(input_var)\n",
    "                \n",
    "                # Compute the loss between the current source descriptor and the target descriptor.\n",
    "                calc_loss = loss(out, target_out)\n",
    "                \n",
    "                # Backward pass: compute gradients with respect to the input image.\n",
    "                calc_loss.backward()\n",
    "                \n",
    "                # Compute the noise update using the sign of the gradient,\n",
    "                # scaled by the small epsilon value.\n",
    "                noise = eps * torch.sign(input_var.grad.data).squeeze()\n",
    "                \n",
    "                # Accumulate noise from this target descriptor.\n",
    "                adv_noise = adv_noise + noise\n",
    "\n",
    "            # Update the source image by subtracting the accumulated adversarial noise.\n",
    "            input_var.data = input_var.data - adv_noise\n",
    "\n",
    "            # Convert the updated image tensor back to CPU and remove the batch dimension.\n",
    "            changed_img = input_var.data.cpu().squeeze()\n",
    "            \n",
    "            # Denormalize the image back to the [0, 255] pixel range.\n",
    "            changed_img = ((changed_img + 0.5) * 255)\n",
    "            \n",
    "            # Clip pixel values to ensure they remain valid.\n",
    "            changed_img[changed_img < 0] = 0\n",
    "            changed_img[changed_img > 255] = 255\n",
    "            \n",
    "            # Convert the tensor image to a numpy array with shape (height, width, channels)\n",
    "            changed_img = np.transpose(changed_img.numpy(), (1, 2, 0)).astype(np.int16)\n",
    "            \n",
    "            # Compute the Structural Similarity Index (SSIM) between the original and changed images.\n",
    "            # This helps ensure the perturbation remains visually imperceptible.\n",
    "            ssim_score = ssim(orig_img, changed_img, channel_axis=2, data_range=256)\n",
    "            \n",
    "            # If the SSIM score drops below 0.95, the perturbation is too visible, so break out of the loop.\n",
    "            if ssim_score < 0.95:\n",
    "                break\n",
    "            else:\n",
    "                # Otherwise, update the attacked_img to the current changed image.\n",
    "                attacked_img = changed_img\n",
    "        \n",
    "        # Save the final attacked image for the given source image in the dictionary.\n",
    "        attacked_img_dict[s] = attacked_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:43:58.264186Z",
     "iopub.status.busy": "2025-03-02T17:43:58.263835Z",
     "iopub.status.idle": "2025-03-02T17:44:04.853613Z",
     "shell.execute_reply": "2025-03-02T17:44:04.852897Z",
     "shell.execute_reply.started": "2025-03-02T17:43:58.264153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pl.read_csv('data/raw/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:44:04.855152Z",
     "iopub.status.busy": "2025-03-02T17:44:04.854859Z",
     "iopub.status.idle": "2025-03-02T17:44:31.078719Z",
     "shell.execute_reply": "2025-03-02T17:44:31.076868Z",
     "shell.execute_reply.started": "2025-03-02T17:44:04.855106Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae1cbcce96146de9c863bfab02ea1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty DataFrame using Polars (pl) for the sample submission.\n",
    "sample_submission_df = pl.DataFrame()\n",
    "\n",
    "# Assign the 'Id' column from an existing sample_submission DataFrame to the new DataFrame.\n",
    "sample_submission_df['Id'] = sample_submission['Id']\n",
    "\n",
    "# Initialize an empty list to hold the result strings for each attacked image.\n",
    "result = []\n",
    "\n",
    "# Iterate over each image ID in the sample_submission DataFrame using tqdm for progress visualization.\n",
    "for id_ in tqdm(sample_submission_df['Id']):\n",
    "    # Retrieve the attacked image from the attacked_img_dict using the current ID.\n",
    "    # Flatten the image array to a 1D list of pixel values.\n",
    "    # Convert each pixel value to a string.\n",
    "    # Join all pixel strings with the '|' character to form a single string.\n",
    "    result += ['|'.join([str(i) for i in attacked_img_dict[id_].flatten().tolist()])]\n",
    "    \n",
    "# Add a new column 'Target' to the DataFrame, containing the processed attacked image pixel values.\n",
    "sample_submission_df['Target'] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:44:31.080333Z",
     "iopub.status.busy": "2025-03-02T17:44:31.079974Z",
     "iopub.status.idle": "2025-03-02T17:44:42.362361Z",
     "shell.execute_reply": "2025-03-02T17:44:42.361659Z",
     "shell.execute_reply.started": "2025-03-02T17:44:31.080299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission_df.to_csv('data/submissions/solution.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:44:42.363367Z",
     "iopub.status.busy": "2025-03-02T17:44:42.363100Z",
     "iopub.status.idle": "2025-03-02T17:44:42.367093Z",
     "shell.execute_reply": "2025-03-02T17:44:42.366356Z",
     "shell.execute_reply.started": "2025-03-02T17:44:42.363346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# METRIC FUNCTION\n",
    "\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "# \n",
    "# class MCSDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, image_path,  imsize = 112):\n",
    "#         self.image_path = image_path\n",
    "#         self.image_size = imsize\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_path)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img = self.image_path[idx]\n",
    "#         img = (img / 255.) - 0.5\n",
    "#         img = np.transpose(img,(2,0,1)).astype(np.float32)\n",
    "#         img = torch.from_numpy(img)\n",
    "\n",
    "#         return img\n",
    "\n",
    "        \n",
    "# model = Model().eval()\n",
    "\n",
    "# pairs = pd.read_csv('data/raw/pairs_list.csv')\n",
    "# paths_embeds = pd.read_csv('data/raw/paths_embeds.csv')\n",
    "# embeds = np.load('data/raw/real_embeds.npy')\n",
    "# sample_submission = pd.read_csv('data/raw/sample_submission.csv')\n",
    "# dict_embeds = {x:i for i,x in enumerate(paths_embeds['image_path'])}\n",
    "\n",
    "# imgs_ = [np.array([int(i) for i in x.split('|')]).reshape((112, 112, 3)) for x in submission['Target']]\n",
    "\n",
    "# dict_ss_ids = {x:i for i,x in enumerate(sample_submission['Id'])}\n",
    "# for i, ids in enumerate(submission['Id']):\n",
    "#     val = sample_submission['Target'][dict_ss_ids[ids]]\n",
    "#     val = np.array([int(i) for i in val.split('|')]).reshape((112, 112, 3))\n",
    "#     sim_ = ssim(imgs_[i], val, channel_axis=2, data_range = 256)\n",
    "#     if sim_ < 0.95:\n",
    "#         return -1\n",
    "        \n",
    "# params_val = {'batch_size': 64, 'shuffle': False, 'drop_last': False, 'num_workers': 2}\n",
    "# imgs_path = os.listdir('/kaggle/input/ioai-contest-2') \n",
    "# val_loader = torch.utils.data.DataLoader(MCSDataset(imgs_), **params_val)\n",
    "\n",
    "# embeds_sourse = []\n",
    "# with torch.no_grad():\n",
    "#     for batch_number,  img  in tqdm(enumerate(val_loader)):\n",
    "#         outputs = model(img, None, train = False)\n",
    "#         embeds_sourse += [outputs]\n",
    "# embeds_sourse = np.concatenate(embeds_sourse)\n",
    "\n",
    "# dict_sours = {x:i for i,x in enumerate(submission['Id'])}\n",
    "# all_paths = set(submission['Id'])\n",
    "\n",
    "# all_score = []\n",
    "# for sour, targ in zip(pairs['source_imgs'], pairs['target_imgs']):\n",
    "\n",
    "#     sour = sour.split('|')\n",
    "#     targ = targ.split('|')\n",
    "\n",
    "#     if sour[0] in all_paths:\n",
    "#         score = []\n",
    "#         for s in sour:\n",
    "#             for t in targ:\n",
    "#                 if t != s:\n",
    "#                     score += [((embeds[dict_embeds[t]] - embeds_sourse[dict_sours[s]]) ** 2).sum() ** (1/2)]\n",
    "                    \n",
    "#         score = np.mean(score)\n",
    "#         all_score += [score]\n",
    "\n",
    "# score = np.mean(score)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11169137,
     "sourceId": 93773,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
