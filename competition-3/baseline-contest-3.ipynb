{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport pandas as pd\nimport numpy as np\nimport string\nimport random\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set params\nseed = 52\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size = 4\ncheckpoint_path = 'unsloth/Llama-3.2-3B-Instruct'\ndebug = False\n\nprompt = '''You are a concise and precise assistant. Answer the questions directly and as briefly as possible.\n           Your answers should be one of the following:\n            1. \"Yes\" if the answer is affirmative.\n            2. \"No\" if the answer is negative.\n            3. \"Insufficient information\" if you don't have enough information to answer.\n            4. The specific entity related to the question (such as a personal name, company, etc.), if applicable.\n\n            Do not explain or provide additional details. Just give the most relevant answer based on the question and your knowledge.\n        '''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/ioai-contest-3/test.csv', index_col=0)\ntest_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create custom dataset\nclass MyDataset(Dataset):\n    def __init__(self, dataset, tokenizer, prompt):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.prompt = prompt\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        example = self.dataset.iloc[idx]\n\n        # form example for LLM\n        input_text = f\"<|system|> {self.prompt}\\n\"\n        input_text += f\"<|user|> {example['questions']}\\n\"\n        input_text += f\"<|assistant|> Answer:\"\n        return idx, input_text\n\ndef collate_fn(batch):\n    idxs, queries = zip(*batch)\n\n    # tokenize batch with padding according to the longest example\n    inputs = tokenizer(\n        list(queries),\n        truncation=True,\n        padding='longest',\n        return_tensors=\"pt\"\n    ).to(device)\n\n    return idxs, queries, inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# init tokenizer and model\n\ntokenizer = AutoTokenizer.from_pretrained(\n            checkpoint_path,\n            trust_remote_code=True\n        )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint_path,\n    device_map=\"auto\",\n    trust_remote_code=True)\nmodel.config.pad_token_id = model.config.eos_token_id","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# init ds, loader\ndataset = MyDataset(test_df, tokenizer, prompt)\ntest_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For answer postprocessing\ndef get_clean_text(text):\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.strip()\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(228)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate answers\nresult_dict = {}\nfor idxs, queries, tokens in tqdm(test_loader):\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=tokens[\"input_ids\"],\n            attention_mask=tokens[\"attention_mask\"],\n            max_new_tokens=256,\n            pad_token_id=model.config.pad_token_id,\n        )\n\n    for num, output in enumerate(outputs):\n        response = tokenizer.decode(outputs[num, tokens[\"input_ids\"][num].shape[0]:], skip_special_tokens=True)\n        pred = get_clean_text(response)\n        result_dict[idxs[num]] = pred\n        if debug:\n            print(f'Model input: {queries[num]}\\n')\n            print(f'Model answer: {pred}\\n\\n')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(result_dict.items(), columns=[\"ID\", \"answer\"]).sort_values(by=\"ID\")\ndf['answer'] = df['answer'].apply(lambda x:x.lower().replace('.', ''))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['answer'] = [x if 'no' not in x.split(' ') else 'no' for x in df['answer']]\ndf['answer'] = [x if 'yes' not in x.split(' ') else 'yes' for x in df['answer']]\ndf.to_csv('baseline.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}